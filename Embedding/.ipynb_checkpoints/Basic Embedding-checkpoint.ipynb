{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db75ebf8",
   "metadata": {},
   "source": [
    "## Basic packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a60d2b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import string\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import nltk, re\n",
    "from nltk.corpus import stopwords\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from datetime import datetime\n",
    "from gensim.models import *\n",
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "sns.set()\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da43a02",
   "metadata": {},
   "source": [
    "## Packages for self design fuction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b55f5069",
   "metadata": {},
   "outputs": [],
   "source": [
    "special_characters = re.compile(\"[^A-Za-z0-9 ]\")\n",
    "tokenizer = nltk.data.load(\"tokenizers/punkt/english.pickle\")\n",
    "def convert_to_sentences(data, tokenizer):\n",
    "    # First, converting each review into sentences\n",
    "    # Use NLTK Tokenizer to split review into sentences (punkt tokenizer - english.pickle)\n",
    "    data = data.lower().replace(\"<br />\", \" \")\n",
    "    data = data.replace(\"-\", \" \")\n",
    "    data = data.replace(\".\", \". \")\n",
    "    data = re.sub(\"  \", \" \", data)\n",
    "    all_sentences = tokenizer.tokenize(data.strip())\n",
    "    \n",
    "    # Second, converting each sentence into words\n",
    "    sentences = []\n",
    "    for words in all_sentences:\n",
    "        s = re.sub(special_characters, \"\", words.lower())\n",
    "        if (len(s)) > 0:\n",
    "            sentences.append(s.split())\n",
    "    \n",
    "    # Finally, returning a list of sentences (containing words in each sentence)\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff439c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "review_A = []\n",
    "for line in open('Toys_and_Games_5.json', 'r'):\n",
    "    review_A.append(json.loads(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "943f6ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(review_A)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c2b7f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = []\n",
    "for r in df.reviewText:\n",
    "    sentences += convert_to_sentences(r, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a32f7e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_feature = 50\n",
    "min_word_count = 20\n",
    "num_thread = 5\n",
    "window_size = 10\n",
    "down_sampling = 0.001\n",
    "iteration = 12\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcbf74e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_w2v = word2vec.Word2Vec(sentences,\n",
    "                          epochs = iteration,\n",
    "                          vector_size=num_feature, \n",
    "                          min_count = min_word_count, \n",
    "                          window = window_size, \n",
    "                          sample = down_sampling, \n",
    "                          workers=num_thread)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b297a049",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_FT = FastText(sentences, \n",
    "                          vector_size=num_feature, \n",
    "                          epochs = iteration,\n",
    "                          window=window_size, \n",
    "                          min_count=min_word_count, \n",
    "                          workers=num_thread)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e53a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_w2v.save(\"gensim_word2vec_withstop\")\n",
    "model_FT.save(\"gensim_fastText_withstop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "614d1e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_top20 = pd.DataFrame({'w2v':model_w2v.wv.index_to_key[:20],\n",
    "                         'FT':model_FT.wv.index_to_key[:20]\n",
    "                        })\n",
    "df_top20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c562bce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.reviewText = df.reviewText.apply(lambda x:x.lower())\n",
    "df.reviewText = df.reviewText.apply(lambda x: \" \".join(x for x in x.split() if x not in stopwords.words('english')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9afb44e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_nostop = []\n",
    "for r in df.reviewText:\n",
    "    sentences_nostop += convert_to_sentences(r, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed357ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in sentences_nostop[0:5]: \n",
    "    print(\"{}\\n\".format(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b9e6bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_w2v_ns = word2vec.Word2Vec(sentences_nostop,\n",
    "                          epochs = iteration,\n",
    "                          vector_size=num_feature, \n",
    "                          min_count = min_word_count, \n",
    "                          window = window_size, \n",
    "                          sample = down_sampling, \n",
    "                          workers=num_thread)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe8efc34",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_FT_ns = FastText(sentences_nostop, \n",
    "                          vector_size=num_feature, \n",
    "                          epochs = iteration,\n",
    "                          window=window_size, \n",
    "                          min_count=min_word_count, \n",
    "                          workers=num_thread)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c1ff8f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_w2v_ns.save(\"gensim_word2vec_nostop\")\n",
    "model_FT_ns.save(\"gensim_fastText_nostop\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
